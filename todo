evaluator
        task = tasks.get_task(task_name)
        dataset = task.get_dataset()
        n_tasks = self.args.limit if self.args.limit else len(generations)

        def tokenize(example):
            inputs = self.tokenizer(
                example,
                padding=True,
                truncation=True,
                return_tensors="pt",
                #max_length=args.max_length_generation,
            )
            return {
                "input_ids": inputs["input_ids"],
                "attention_mask": inputs["attention_mask"]
            }

        # for results saving
        result = {"z_threshold": self.args.detection_z_threshold}
        detect_list = []
        ent_list = []
        detection_results = []
        len_list = []

        prompt_contents = [task.get_prompt(dataset[sample]) for sample in range(n_tasks)]

            for idx, gens in tqdm(enumerate(generations), total=len(generations)):
                n_detection = 0
                for idx2, gen in enumerate(gens):
                     .....

這邊要處理prefix的問題
e.g
prompt_contents[idx] + gen (<-rename這邊)

